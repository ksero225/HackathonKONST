import os
import json
import threading
import time
from textwrap import dedent
from typing import Optional, List, Dict

from dotenv import load_dotenv
from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI

import websocket  # klient WebSocket/STOMP
import requests   # HTTP do backendu Javy
import re
import numpy as np

# ================== KONFIG OPENAI / .ENV ==================
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("Brak zmiennej Å›rodowiskowej OPENAI_API_KEY (sprawdÅº plik .env)")

client = OpenAI(api_key=OPENAI_API_KEY)

# JAVA_BASE_URL z .env
JAVA_BASE_URL = os.getenv("JAVA_BASE_URL")
if not JAVA_BASE_URL:
    raise RuntimeError("Brak zmiennej Å›rodowiskowej JAVA_BASE_URL (dodaj do .env)")

app = FastAPI(
    title="ProfilBot Conversation API",
    description="Dynamiczny bot do rozmowy i iteracyjnego budowania opisu profilu",
    version="0.4.1",
)

# ================== STAN ROZMÃ“W (per userId) ==================


class QA(BaseModel):
    question: str
    answer: str


class ChatState(BaseModel):
    description: str = ""
    transcript: List[QA] = []
    last_question: Optional[str] = None
    finished: bool = False


user_states: Dict[int, ChatState] = {}
states_lock = threading.Lock()

# ================== STOMP / WEBSOCKET ==================


def stomp_frame(command, headers=None, body: str = "") -> str:
    if headers is None:
        headers = {}
    frame = command + "\n"
    for key, value in headers.items():
        frame += f"{key}:{value}\n"
    frame += "\n" + body + "\0"
    return frame


class WebSocketClient:
    """
    Klient WebSocket z komunikacjÄ… STOMP.
    Odbiera wiadomoÅ›ci uÅ¼ytkownikÃ³w i wysyÅ‚a odpowiedzi AI
    jako DescriptionChatMessage do backendu Javy.
    """

    def __init__(self, uri: str):
        self.uri = uri
        self.ws = None
        self.connected = False
        self.running = True
        self.thread = threading.Thread(target=self._run_loop)
        self.thread.daemon = True

    def start(self):
        self.thread.start()
        print("ğŸš€ [WS] Klient WebSocket uruchomiony w tle.")

    def _run_loop(self):
        while self.running:
            try:
                self.ws = websocket.WebSocketApp(
                    self.uri,
                    on_open=self.on_open,
                    on_message=self.on_message,
                    on_error=self.on_error,
                    on_close=self.on_close,
                )
                self.ws.run_forever()
            except Exception as e:
                print(f"âš ï¸ [WS] BÅ‚Ä…d wÄ…tku: {e}")
                time.sleep(3)

            if self.running:
                print("ğŸ”„ [WS] PrÃ³ba ponownego poÅ‚Ä…czenia za 3s...")
                time.sleep(3)

    def on_open(self, ws):
        print("âœ… [WS] PoÅ‚Ä…czono z serwerem.")
        self.connected = True

        connect_frame = stomp_frame(
            "CONNECT",
            headers={
                "accept-version": "1.1,1.2",
                "host": "localhost",
            },
        )
        ws.send(connect_frame)
        time.sleep(0.5)

        sub_frame = stomp_frame(
            "SUBSCRIBE",
            headers={
                "id": "sub-desc-0",
                "destination": "/topic/description",
            },
        )
        ws.send(sub_frame)
        print("ğŸ§ [WS] Zasubskrybowano /topic/description")

    def on_message(self, ws, message):
        if message == "\n":
            return

        if "\n\n" not in message:
            print(f"ğŸ“© [WS RAW]: {message!r}")
            return

        header, body = message.split("\n\n", 1)
        clean_body = body.replace("\x00", "").strip()

        if not clean_body:
            return

        try:
            data = json.loads(clean_body)

            status_code = data.get("statusCode")
            status_value = data.get("statusCodeValue")
            resp_body = data.get("body", {}) or {}

            user_id = resp_body.get("userId")
            content = resp_body.get("content")

            print("ğŸ“© [WS ODBIÃ“R]:")
            print(f"   status: {status_code} ({status_value})")
            print(f"   userId: {user_id}")
            print(f"   content: {content!r}")

            # rozpoznanie czy to nasza wiadomoÅ›Ä‡ AI (Å¼eby nie zrobiÄ‡ pÄ™tli)
            is_ai_message = False
            if isinstance(content, str):
                try:
                    parsed = json.loads(content)
                    if isinstance(parsed, dict) and parsed.get("type") == "AI":
                        is_ai_message = True
                except json.JSONDecodeError:
                    pass

            if is_ai_message:
                print("â„¹ï¸ [WS] Otrzymano wiadomoÅ›Ä‡ AI â€“ pomijam (Å¼eby nie wejÅ›Ä‡ w pÄ™tlÄ™).")
                return

            if user_id is not None and isinstance(content, str):
                handle_user_message(user_id=int(user_id), user_text=content)
            else:
                print("âš ï¸ [WS] Brak userId lub content nie jest tekstem â€“ pomijam.")

        except json.JSONDecodeError:
            print(f"âš ï¸ [WS] Nie udaÅ‚o siÄ™ zparsowaÄ‡ JSON: {clean_body!r}")
        except Exception as e:
            print(f"âŒ [WS] BÅ‚Ä…d obsÅ‚ugi wiadomoÅ›ci: {e}")

    def on_error(self, ws, error):
        print(f"âŒ [WS] BÅ‚Ä…d: {error}")

    def on_close(self, ws, close_status_code, close_msg):
        print("ğŸ”Œ [WS] RozÅ‚Ä…czono.")
        self.connected = False

    def send_description(self, user_id: int, content_string: str):
        """
        WysyÅ‚a DescriptionChatMessage przez STOMP:
        { "userId": ..., "content": "..." }
        """
        print(
            f"[DEBUG] send_description() called, user_id={user_id}, "
            f"connected={self.connected}, ws_is_not_none={self.ws is not None}"
        )

        if not user_id:
            print("âš ï¸ [WS] Brak user_id - nie wysyÅ‚am wiadomoÅ›ci STOMP.")
            return

        if self.ws and self.connected:
            try:
                payload = {
                    "userId": user_id,
                    "content": content_string,
                }
                body = json.dumps(payload, ensure_ascii=False)
                body_bytes = body.encode("utf-8")

                send_frame = stomp_frame(
                    "SEND",
                    headers={
                        "destination": "/app/description",
                        "content-type": "application/json;charset=UTF-8",
                        "content-length": str(len(body_bytes)),
                    },
                    body=body,
                )

                print(f"[WS] STOMP frame body: {body}")
                self.ws.send(send_frame)
                print(
                    f"ğŸ“¤ [WS WYSÅANO] User: {user_id} | Content len: {len(content_string)}"
                )
            except Exception as e:
                print(f"âŒ [WS] BÅ‚Ä…d wysyÅ‚ania: {e}")
        else:
            print("âš ï¸ [WS] Nie moÅ¼na wysÅ‚aÄ‡ - brak poÅ‚Ä…czenia.")


WS_URI = os.getenv("WS_URI", "wss://continuable-manuela-podgy.ngrok-free.dev/ws")
ws_client = WebSocketClient(WS_URI)
ws_client.start()

# ================== MODEL CECH ==================


class ProfileFeatures(BaseModel):
    activities: List[str] = []
    style_intensity: Optional[str] = None
    style_competition: Optional[str] = None
    group_size: Optional[str] = None
    atmosphere: Optional[str] = None
    location_hint: Optional[str] = None
    tags: List[str] = []

    lat: Optional[float] = None
    lon: Optional[float] = None


def normalize_feature_name(name: str) -> str:
    if not name:
        return name
    name = name.strip().lower()
    name = re.sub(r"\s+", " ", name)
    name = name.replace(" ", "_")
    name = re.sub(r"[^0-9a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼_]", "", name)
    return name


def normalize_profile_features(p: ProfileFeatures) -> ProfileFeatures:
    return ProfileFeatures(
        activities=[normalize_feature_name(a) for a in p.activities],
        style_intensity=p.style_intensity,
        style_competition=p.style_competition,
        group_size=p.group_size,
        atmosphere=p.atmosphere,
        location_hint=p.location_hint,
        tags=[normalize_feature_name(t) for t in p.tags],
        lat=p.lat,
        lon=p.lon,
    )


def extract_features_from_description(description: str) -> ProfileFeatures:
    system_prompt = dedent("""
    JesteÅ› asystentem, ktÃ³ry z gotowego opisu profilu uÅ¼ytkownika wyciÄ…ga cechy do grupowania ludzi w aplikacji spoÅ‚ecznoÅ›ciowej.

    TWOJE ZADANIE:
    - Przeczytaj opis profilu (po polsku).
    - WyciÄ…gnij z niego kluczowe informacje:
      * aktywnoÅ›ci / zainteresowania (lista fraz),
      * styl/intensywnoÅ›Ä‡: bardziej spokojnie czy ambitnie,
      * czy szuka raczej chillowego klimatu, czy rywalizacji,
      * w jakiej wielkoÅ›ci grupie czuje siÄ™ najlepiej,
      * jaki klimat spotkaÅ„ preferuje (spokojnie/energicznie),
      * co da siÄ™ wywnioskowaÄ‡ o lokalizacji / typowych miejscach,
      * proste tagi (np. "bieganie", "planszÃ³wki", "kawa", "maÅ‚a_grupa", "spokojnie", itp.).

    ZWRÃ“Ä† TYLKO CZYSTY JSON O STRUKTURZE:
    {
      "activities": [lista stringÃ³w],
      "style_intensity": "spokojnie" | "ambitnie" | "mieszane" | null,
      "style_competition": "chill" | "rywalizacja" | "mieszane" | null,
      "group_size": "maÅ‚e" | "Å›rednie" | "duÅ¼e" | null,
      "atmosphere": "spokojnie" | "energicznie" | "mieszane" | null,
      "location_hint": string lub null,
      "tags": [lista stringÃ³w]
    }

    Nie dodawaj Å¼adnych komentarzy poza JSON-em.
    """).strip()

    user_prompt = (
        f"Oto opis profilu uÅ¼ytkownika:\n\n{description}\n\n"
        f"WyodrÄ™bnij cechy zgodnie z formatem."
    )

    print(f"[LOG] Ekstrakcja cech z opisu (finalDescription):\n{description}\n")

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.0,
        max_tokens=300,
    )

    content = response.choices[0].message.content.strip()
    print(f"[DEBUG] OpenAI (extract_features) raw content: {content!r}")

    try:
        data = json.loads(content)
    except Exception:
        print("âš ï¸ Model zwrÃ³ciÅ‚ coÅ›, co nie jest JSON-em. Zwracam puste cechy.")
        return ProfileFeatures()

    features = ProfileFeatures(
        activities=data.get("activities") or [],
        style_intensity=data.get("style_intensity"),
        style_competition=data.get("style_competition"),
        group_size=data.get("group_size"),
        atmosphere=data.get("atmosphere"),
        location_hint=data.get("location_hint"),
        tags=data.get("tags") or [],
    )

    print(f"[LOG] Surowe cechy wyekstrahowane z opisu: {features}")
    return features


def is_sparse_description(description: str) -> bool:
    return len(description.split()) < 5


def build_tag_index(profiles: List[ProfileFeatures]) -> Dict[str, int]:
    all_tags = set()
    for p in profiles:
        all_tags.update(p.tags)
        all_tags.update(p.activities)
    tag_index = {tag: i for i, tag in enumerate(sorted(all_tags))}
    print(f"[LOG] Zbudowano tag_index (tag -> index): {tag_index}")
    return tag_index


def profile_to_vector(profile: ProfileFeatures, tag_index: Dict[str, int]) -> np.ndarray:
    v = np.zeros(len(tag_index), dtype=float)

    for act in profile.activities:
        if act in tag_index:
            idx = tag_index[act]
            v[idx] = max(v[idx], 1.0)

    for tag in profile.tags:
        if tag in tag_index:
            idx = tag_index[tag]
            v[idx] = max(v[idx], 0.7)

    if profile.style_intensity == "ambitnie":
        v *= 1.1

    max_val = v.max()
    if max_val > 0:
        v = v / max_val

    print(f"[LOG] Wektor cech (bez sparse): {v}")
    return v


def profile_to_vector_with_sparse_flag(
    profile: ProfileFeatures,
    tag_index: Dict[str, int],
    description: str,
) -> np.ndarray:
    v = profile_to_vector(profile, tag_index)
    if is_sparse_description(description):
        print("[LOG] Opis jest krÃ³tki ('skÄ…py') â€“ obniÅ¼am wagi x0.5")
        v *= 0.5
    print(f"[LOG] Wektor cech (po sparse flag): {v}")
    return v


def vector_to_readable_dict(v: np.ndarray, tag_index: Dict[str, int]) -> Dict[str, float]:
    idx_to_tag = {idx: tag for tag, idx in tag_index.items()}
    result = {}
    for idx, val in enumerate(v):
        if val > 0:
            tag = idx_to_tag[idx]
            result[tag] = round(float(val), 2)
    print(f"[LOG] traits (tag -> wartoÅ›Ä‡): {result}")
    return result


def send_final_description_to_backend(user_id: int, final_description: str):
    """
    1) WyciÄ…ga cechy z final_description,
    2) Buduje wektor,
    3) Zamienia na mapÄ™ traits,
    4) WysyÅ‚a POST na /api/users/{userId}/description
    + LOGI na kaÅ¼dym etapie.
    """
    print("\n================= [FLOW] ZAPIS OPISU DO BACKENDU =================")
    print(f"[FLOW] user_id={user_id}")
    print(f"[FLOW] finalDescription:\n{final_description}\n")

    try:
        # 1. Ekstrakcja cech
        raw_features = extract_features_from_description(final_description)
        print(f"[LOG] raw_features (przed normalizacjÄ…): {raw_features}")

        features = normalize_profile_features(raw_features)
        print(f"[LOG] features (po normalizacji): {features}")

        # 2. Tag index na podstawie cech tego uÅ¼ytkownika
        tag_index = build_tag_index([features])

        # 3. Wektor
        vec = profile_to_vector_with_sparse_flag(features, tag_index, final_description)

        # 4. traits (czytelny sÅ‚ownik)
        traits = vector_to_readable_dict(vec, tag_index)

        payload = {
            "text": final_description,
            "traits": traits,
        }

        url = f"{JAVA_BASE_URL}/api/users/{user_id}/description"
        print(f"[HTTP] POST {url}")
        print(f"[HTTP] Payload JSON: {json.dumps(payload, ensure_ascii=False)}")

        resp = requests.put(url, json=payload)
        print(f"[HTTP] OdpowiedÅº backendu: {resp.status_code}")
        print(f"[HTTP] Body odpowiedzi: {resp.text}")
        print("================= [FLOW] ZAPIS OPISU ZAKOÅƒCZONY =================\n")

    except Exception as e:
        print(f"âŒ [HTTP] BÅ‚Ä…d wysyÅ‚ania opisu do backendu: {e}")


# ================== FUNKCJA BOTA â€“ iteracyjne budowanie opisu ==================


def refine_description_with_openai(
    description: str,
    transcript: List[QA],
    last_question: str,
    last_answer: str,
) -> tuple[str, bool, Optional[str]]:
    system_prompt = dedent(
        """
        JesteÅ› asystentem, ktÃ³ry prowadzi rozmowÄ™ z uÅ¼ytkownikiem i na jej podstawie
        buduje opis profilu do aplikacji spoÅ‚ecznoÅ›ciowej.

        TEN OPIS MA BYÄ† PODSTAWÄ„ DO WYCIÄ„GANIA CECH DO GRUPOWANIA UÅ»YTKOWNIKÃ“W:
        - rodzaje aktywnoÅ›ci / zainteresowaÅ„ (ale nie pytaj o to zawsze wprost),
        - styl / intensywnoÅ›Ä‡ (luÅºno vs ambitnie, rywalizacja vs chill),
        - preferowany typ grupy i atmosfery (maÅ‚e grupÑ‹ vs wiÄ™ksze, spokojnie vs gÅ‚oÅ›no),
        - ogÃ³lna lokalizacja / kontekst (np. centrum, dzielnica, miasto, typ miejsc).

        WAÅ»NE:
        - Twoje pytania nie mogÄ… byÄ‡ caÅ‚y czas takie same.
        - Nie pytaj tylko sucho o "aktywnoÅ›ci".
        - MoÅ¼esz dopytywaÄ‡ o:
          * przykÅ‚adowe sytuacje ("jak wyglÄ…da idealne spotkanie z ludÅºmi?"),
          * towarzystwo ("z jakimi osobami najlepiej siÄ™ dogadujesz?"),
          * klimat ("raczej gÅ‚oÅ›ne miejsca czy spokojne rozmowy?"),
          * miejsca ("bardziej parki, miasto, kawiarnie, Å›cianka wspinaczkowa?").
        - Pytania majÄ… brzmieÄ‡ naturalnie i po ludzku, po polsku.

        TWOJE ZADANIE W TYM KROKU:
        1) WeÅº dotychczasowy opis oraz ostatniÄ… odpowiedÅº uÅ¼ytkownika i zaktualizuj opis tak, aby:
           - byÅ‚ spÃ³jny,
           - zawieraÅ‚ kluczowe informacje z caÅ‚ej rozmowy,
           - miaÅ‚ maksymalnie 3â€“4 zdania.
        2) OceÅ„, czy na podstawie tego opisu da siÄ™ juÅ¼ zbudowaÄ‡ sensowny wektor cech do grupowania uÅ¼ytkownikÃ³w
           (aktywnoÅ›ci, styl, grupa, klimat, miejsca).
           JeÅ›li tak â†’ sufficient = true.
           JeÅ›li nie â†’ sufficient = false i wygeneruj jedno konkretne, DOPEÅNIAJÄ„CE pytanie.

        FORMAT ODPOWIEDZI:
        ZwrÃ³Ä‡ TYLKO czysty JSON, bez Å¼adnych komentarzy, bez markdown:
        {
          "new_description": "...",
          "sufficient": true/false,
          "next_question": "..." albo null
        }
        """
    ).strip()

    if transcript:
        conv_lines = []
        for i, qa in enumerate(transcript, start=1):
            conv_lines.append(f"Pytanie {i}: {qa.question}")
            conv_lines.append(f"OdpowiedÅº {i}: {qa.answer}")
        conv_text = "\n".join(conv_lines)
    else:
        conv_text = "(Brak wczeÅ›niejszych pytaÅ„ i odpowiedzi - to poczÄ…tek rozmowy.)"

    user_prompt = dedent(
        f"""
        DOTYCHCZASOWY OPIS PROFILU:
        {description or "(brak opisu - tworzysz go od zera)"}

        HISTORIA ROZMOWY:
        {conv_text}

        OSTATNI KROK:
        Pytanie: {last_question}
        OdpowiedÅº: {last_answer}

        Na tej podstawie:
        1) Zaktualizuj opis (pole new_description),
        2) OceÅ„ sufficient (czy opis jest juÅ¼ wystarczajÄ…cy),
        3) JeÅ›li nie, zaproponuj next_question - naturalne, konkretne pytanie,
           ktÃ³re pomoÅ¼e dodaÄ‡ brakujÄ…ce informacje (ale nie musi byÄ‡ wprost o "zainteresowaniach").
        PAMIÄ˜TAJ: zwracasz tylko surowy JSON, bez innych treÅ›ci.
        """
    ).strip()

    print("[FLOW] refine_description_with_openai() â€“ wywoÅ‚anie modelu")
    print(f"[FLOW] last_question: {last_question}")
    print(f"[FLOW] last_answer:  {last_answer}")
    print(f"[FLOW] prev_description: {description}\n")

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.4,
        max_tokens=300,
    )

    raw_content = response.choices[0].message.content.strip()
    print(f"[DEBUG] OpenAI (refine) raw content: {raw_content!r}")

    json_str = None
    if raw_content.startswith("{"):
        json_str = raw_content
    else:
        start = raw_content.find("{")
        end = raw_content.rfind("}")
        if start != -1 and end != -1 and end > start:
            json_str = raw_content[start: end + 1]

    try:
        if not json_str:
            raise ValueError("Nie udaÅ‚o siÄ™ wyodrÄ™bniÄ‡ JSON z odpowiedzi modelu.")
        data = json.loads(json_str)

        new_description = data.get("new_description", description) or description
        sufficient = bool(data.get("sufficient", False))
        next_question = data.get("next_question")

        print(f"[LOG] new_description: {new_description}")
        print(f"[LOG] sufficient: {sufficient}")
        print(f"[LOG] next_question: {next_question}\n")

    except Exception as e:
        print(f"[WARN] BÅ‚Ä…d parsowania JSON z OpenAI: {e}. raw_content={raw_content!r}")
        new_description = description or last_answer
        sufficient = False
        next_question = (
            "Czy mÃ³gÅ‚byÅ› opisaÄ‡ trochÄ™ dokÅ‚adniej, z jakimi ludÅºmi i w jakich miejscach "
            "najbardziej lubisz spÄ™dzaÄ‡ czas?"
        )

    if len(transcript) < 2:
        print("[LOG] Mniej niÅ¼ 2 Q/A â€“ sufficient ustawione na False (za maÅ‚o danych).")
        sufficient = False

    if not sufficient and (not next_question or next_question.strip() == ""):
        next_question = (
            "Dodaj proszÄ™ jeszcze coÅ› o tym, jakie sytuacje lubisz najbardziej "
            "np. spokojne rozmowy w kawiarni, wypad w gÃ³ry, planszÃ³wki, sporty itp."
        )
        print("[LOG] Brak next_question â€“ ustawiam awaryjne pytanie.")

    return new_description, sufficient, next_question


# ================== GÅÃ“WNA LOGIKA â€“ OBSÅUGA WIADOMOÅšCI UÅ»YTKOWNIKA ==================


def handle_user_message(user_id: int, user_text: str):
    print(f"\n[FLOW] handle_user_message(user_id={user_id}, user_text={user_text!r})")

    with states_lock:
        state = user_states.get(user_id)
        if state is None or state.finished:
            print(f"[LOG] TworzÄ™ nowy ChatState dla user_id={user_id}")
            state = ChatState()
            user_states[user_id] = state

    # Pierwsza wiadomoÅ›Ä‡
    if not state.last_question and not state.transcript:
        print("[FLOW] Pierwsza wiadomoÅ›Ä‡ w rozmowie (syntetyczne pytanie startowe).")
        synthetic_question = (
            "Na poczÄ…tek opisz w 2â€“3 zdaniach, jak lubisz spÄ™dzaÄ‡ czas z innymi ludÅºmi "
            "i czego szukasz w takich spotkaniach?"
        )
        qa = QA(question=synthetic_question, answer=user_text)
        state.transcript.append(qa)

        new_description, sufficient, next_question = refine_description_with_openai(
            description=state.description,
            transcript=state.transcript,
            last_question=synthetic_question,
            last_answer=user_text,
        )

        state.description = new_description

        if sufficient:
            print("[FLOW] Model uznaÅ‚ opis za wystarczajÄ…cy juÅ¼ po pierwszej wiadomoÅ›ci.")
            state.finished = True
            state.last_question = None

            ai_message = {
                "type": "AI",
                "finished": True,
                "finalDescription": new_description,
            }

            print("[FLOW] WysyÅ‚am finalDescription po WebSocket...")
            ws_client.send_description(
                user_id=user_id,
                content_string=json.dumps(ai_message, ensure_ascii=False),
            )

            print("[FLOW] WywoÅ‚ujÄ™ zapis finalDescription + traits do backendu...")
            send_final_description_to_backend(user_id, new_description)

        else:
            print("[FLOW] Model potrzebuje wiÄ™cej danych â€“ wysyÅ‚am pytanie doprecyzowujÄ…ce.")
            state.finished = False
            state.last_question = next_question

            ai_message = {
                "type": "AI",
                "finished": False,
                "botMessage": next_question,
                "currentDescription": new_description,
            }

            ws_client.send_description(
                user_id=user_id,
                content_string=json.dumps(ai_message, ensure_ascii=False),
            )

        with states_lock:
            user_states[user_id] = state
        return

    # Kolejne wiadomoÅ›ci
    if not state.last_question:
        print(f"âš ï¸ [BOT] Brak last_question dla user_id={user_id}. ResetujÄ™ stan.")
        state = ChatState()
        with states_lock:
            user_states[user_id] = state
        handle_user_message(user_id=user_id, user_text=user_text)
        return

    print(f"[FLOW] OdpowiedÅº na pytanie: {state.last_question!r}")
    qa = QA(question=state.last_question, answer=user_text)
    state.transcript.append(qa)

    new_description, sufficient, next_question = refine_description_with_openai(
        description=state.description,
        transcript=state.transcript,
        last_question=state.last_question,
        last_answer=user_text,
    )

    state.description = new_description

    if sufficient:
        print("[FLOW] Model uznaÅ‚, Å¼e opis jest juÅ¼ wystarczajÄ…cy â€“ koÅ„czÄ™ rozmowÄ™.")
        state.finished = True
        state.last_question = None

        ai_message = {
            "type": "AI",
            "finished": True,
            "finalDescription": new_description,
        }

        print("[FLOW] WysyÅ‚am finalDescription po WebSocket...")
        ws_client.send_description(
            user_id=user_id,
            content_string=json.dumps(ai_message, ensure_ascii=False),
        )

        print("[FLOW] WywoÅ‚ujÄ™ zapis finalDescription + traits do backendu...")
        send_final_description_to_backend(user_id, new_description)
    else:
        print("[FLOW] Model nadal potrzebuje doprecyzowania â€“ zadajÄ™ kolejne pytanie.")
        state.finished = False
        state.last_question = next_question

        ai_message = {
            "type": "AI",
            "finished": False,
            "botMessage": next_question,
            "currentDescription": new_description,
        }

        ws_client.send_description(
            user_id=user_id,
            content_string=json.dumps(ai_message, ensure_ascii=False),
        )

    with states_lock:
        user_states[user_id] = state


# ================== ENDPOINT DIAGNOSTYCZNY ==================


@app.get("/")
def root():
    return {
        "status": "ok",
        "message": "ProfilBot Conversation API dziaÅ‚a (iteracyjny opis, WebSocket-driven, traits->Java)",
    }
